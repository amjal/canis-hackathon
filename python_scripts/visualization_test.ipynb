{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all user JSON files in a directory into a single CSV file\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def extract_user_data(json_data):\n",
    "    user_data = json_data['data']['user']['result']\n",
    "    legacy_data = user_data.get('legacy', {})\n",
    "    professional_data = user_data.get('professional', {})\n",
    "\n",
    "    # Flatten the nested fields in legacy_data and professional_data\n",
    "    combined_data = {\n",
    "        'is_blue_verified': user_data.get('is_blue_verified', False),\n",
    "        'created_at': legacy_data.get('created_at', ''),\n",
    "        'description': legacy_data.get('description', ''),\n",
    "        'favourites_count': legacy_data.get('favourites_count', 0),\n",
    "        'followers_count': legacy_data.get('followers_count', 0),\n",
    "        'friends_count': legacy_data.get('friends_count', 0),\n",
    "        'listed_count': legacy_data.get('listed_count', 0),\n",
    "        'location': legacy_data.get('location', ''),\n",
    "        'media_count': legacy_data.get('media_count', 0),\n",
    "        'name': legacy_data.get('name', ''),\n",
    "        'normal_followers_count': legacy_data.get('normal_followers_count', 0),\n",
    "        'pinned_tweet_ids_str': legacy_data.get('pinned_tweet_ids_str', []),\n",
    "        'possibly_sensitive': legacy_data.get('possibly_sensitive', False),\n",
    "        'profile_banner_url': legacy_data.get('profile_banner_url', ''),\n",
    "        'profile_image_url_https': legacy_data.get('profile_image_url_https', ''),\n",
    "        'screen_name': legacy_data.get('screen_name', ''),\n",
    "        'statuses_count': legacy_data.get('statuses_count', 0),\n",
    "        'verified': legacy_data.get('verified', False),\n",
    "        'want_retweets': legacy_data.get('want_retweets', False),\n",
    "        'withheld_in_countries': legacy_data.get('withheld_in_countries', []),\n",
    "        # Professional data\n",
    "        'professional_type': professional_data.get('professional_type', ''),\n",
    "        # Add more fields from professional_data if needed\n",
    "        # Other user data fields\n",
    "        'smart_blocked_by': user_data.get('smart_blocked_by', False),\n",
    "        'smart_blocking': user_data.get('smart_blocking', False),\n",
    "        'has_hidden_likes_on_profile': user_data.get('has_hidden_likes_on_profile', False),\n",
    "        'has_hidden_subscriptions_on_profile': user_data.get('has_hidden_subscriptions_on_profile', False),\n",
    "        'is_identity_verified': user_data.get('verification_info', {}).get('is_identity_verified', False),\n",
    "        'verified_since_msec': user_data.get('verification_info', {}).get('reason', {}).get('verified_since_msec'),\n",
    "        'can_highlight_tweets': user_data.get('highlights_info', {}).get('can_highlight_tweets', False),\n",
    "        'highlighted_tweets': user_data.get('highlights_info', {}).get('highlighted_tweets', '0'),\n",
    "        'creator_subscriptions_count': user_data.get('creator_subscriptions_count', 0)\n",
    "    }\n",
    "\n",
    "    # Flatten entities' URLs\n",
    "    urls = legacy_data.get('entities', {}).get('url', {}).get('urls', [])\n",
    "    for i, url_info in enumerate(urls):\n",
    "        combined_data[f'url_{i}_display'] = url_info.get('display_url', '')\n",
    "        combined_data[f'url_{i}_expanded'] = url_info.get('expanded_url', '')\n",
    "        combined_data[f'url_{i}_url'] = url_info.get('url', '')\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_data = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            json_data = read_json_file(file_path)\n",
    "            user_data = extract_user_data(json_data)\n",
    "            all_data.append(user_data)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "directory = './twitter/users/'\n",
    "\n",
    "# Process the JSON files and create a DataFrame\n",
    "df = process_json_files(directory)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('users.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Network Graph of Followers and Following\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def extract_following_data(user_name, json_data):\n",
    "    user_following_list = []\n",
    "    instructions = json_data['data']['user']['result']['timeline']['timeline']['instructions']\n",
    "    for instruction in instructions:\n",
    "        if instruction['type'] == 'TimelineAddEntries':\n",
    "            following_entries = instruction['entries']\n",
    "            for entry in following_entries:\n",
    "                if 'content' in entry and 'itemContent' in entry['content']:\n",
    "                    following_user = entry['content']['itemContent'].get('user_results', {}).get('result', {}).get('legacy', {}).get('screen_name', '')\n",
    "                    if following_user:\n",
    "                        user_following_list.append({'user': user_name, 'following': following_user})\n",
    "\n",
    "    return user_following_list\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_following_data = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            json_data = read_json_file(file_path)\n",
    "            following_data = extract_following_data(file.split('-')[0], json_data)\n",
    "            all_following_data.extend(following_data)\n",
    "\n",
    "    return pd.DataFrame(all_following_data)\n",
    "\n",
    "directory = './twitter/followers/'\n",
    "\n",
    "df = process_json_files(directory)\n",
    "\n",
    "df.to_csv('user_following.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all other users from following file into a single CSV file\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def extract_user_data(json_data):\n",
    "    user_data = json_data['result']\n",
    "    legacy_data = user_data.get('legacy', {})\n",
    "    professional_data = user_data.get('professional', {})\n",
    "\n",
    "    # Flatten the nested fields in legacy_data and professional_data\n",
    "    combined_data = {\n",
    "        'is_blue_verified': user_data.get('is_blue_verified', False),\n",
    "        'created_at': legacy_data.get('created_at', ''),\n",
    "        'description': legacy_data.get('description', ''),\n",
    "        'favourites_count': legacy_data.get('favourites_count', 0),\n",
    "        'followers_count': legacy_data.get('followers_count', 0),\n",
    "        'friends_count': legacy_data.get('friends_count', 0),\n",
    "        'listed_count': legacy_data.get('listed_count', 0),\n",
    "        'location': legacy_data.get('location', ''),\n",
    "        'media_count': legacy_data.get('media_count', 0),\n",
    "        'name': legacy_data.get('name', ''),\n",
    "        'normal_followers_count': legacy_data.get('normal_followers_count', 0),\n",
    "        'pinned_tweet_ids_str': legacy_data.get('pinned_tweet_ids_str', []),\n",
    "        'possibly_sensitive': legacy_data.get('possibly_sensitive', False),\n",
    "        'profile_banner_url': legacy_data.get('profile_banner_url', ''),\n",
    "        'profile_image_url_https': legacy_data.get('profile_image_url_https', ''),\n",
    "        'screen_name': legacy_data.get('screen_name', ''),\n",
    "        'statuses_count': legacy_data.get('statuses_count', 0),\n",
    "        'verified': legacy_data.get('verified', False),\n",
    "        'want_retweets': legacy_data.get('want_retweets', False),\n",
    "        'withheld_in_countries': legacy_data.get('withheld_in_countries', []),\n",
    "        # Professional data\n",
    "        'professional_type': professional_data.get('professional_type', ''),\n",
    "        # Add more fields from professional_data if needed\n",
    "        # Other user data fields\n",
    "        'smart_blocked_by': user_data.get('smart_blocked_by', False),\n",
    "        'smart_blocking': user_data.get('smart_blocking', False),\n",
    "        'has_hidden_likes_on_profile': user_data.get('has_hidden_likes_on_profile', False),\n",
    "        'has_hidden_subscriptions_on_profile': user_data.get('has_hidden_subscriptions_on_profile', False),\n",
    "        'is_identity_verified': user_data.get('verification_info', {}).get('is_identity_verified', False),\n",
    "        'verified_since_msec': user_data.get('verification_info', {}).get('reason', {}).get('verified_since_msec'),\n",
    "        'can_highlight_tweets': user_data.get('highlights_info', {}).get('can_highlight_tweets', False),\n",
    "        'highlighted_tweets': user_data.get('highlights_info', {}).get('highlighted_tweets', '0'),\n",
    "        'creator_subscriptions_count': user_data.get('creator_subscriptions_count', 0)\n",
    "    }\n",
    "\n",
    "    # Flatten entities' URLs\n",
    "    urls = legacy_data.get('entities', {}).get('url', {}).get('urls', [])\n",
    "    for i, url_info in enumerate(urls):\n",
    "        combined_data[f'url_{i}_display'] = url_info.get('display_url', '')\n",
    "        combined_data[f'url_{i}_expanded'] = url_info.get('expanded_url', '')\n",
    "        combined_data[f'url_{i}_url'] = url_info.get('url', '')\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def extract_following_data(user_name, json_data):\n",
    "    all_data = []\n",
    "    instructions = json_data['data']['user']['result']['timeline']['timeline']['instructions']\n",
    "    for instruction in instructions:\n",
    "        if instruction['type'] == 'TimelineAddEntries':\n",
    "            following_entries = instruction['entries']\n",
    "            for entry in following_entries:\n",
    "                if 'content' in entry and 'itemContent' in entry['content']:\n",
    "                    following_user = entry['content']['itemContent'].get('user_results', {})\n",
    "                    if following_user:\n",
    "                        user_data = extract_user_data(following_user)\n",
    "                        all_data.append(user_data)\n",
    "    return all_data\n",
    "\n",
    "def process_json_files(directory):\n",
    "    all_following_data = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            json_data = read_json_file(file_path)\n",
    "            following_data = extract_following_data(file.split('-')[0], json_data)\n",
    "            all_following_data.extend(following_data)\n",
    "\n",
    "    return pd.DataFrame(all_following_data)\n",
    "\n",
    "directory = './twitter/followers/'\n",
    "\n",
    "df = process_json_files(directory)\n",
    "\n",
    "df.to_csv('users_other.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_others_df (28891, 33)\n",
      "users_others_df (5784, 33)\n",
      "network_df (10000, 2)\n",
      "network_df (2845, 3)\n",
      "               user    following  following_in_degree\n",
      "5   Chinamission2un       nypost                    1\n",
      "6   Chinamission2un      thehill                    1\n",
      "9   Chinamission2un          NPR                    2\n",
      "10  Chinamission2un         VP45                    3\n",
      "13  Chinamission2un  LeoDiCaprio                    2\n",
      "network_df (214, 3)\n",
      "twitter_network.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/kc2mt7ts2934ygy6lzf8rzwr0000gn/T/ipykernel_78978/3066501456.py:5: DtypeWarning: Columns (0,4,5,6,8,10,12,16,17,18,21,22,23,24,25,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  users_others_df = pd.read_csv('users_other.csv')\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "users_df = pd.read_csv('users.csv')\n",
    "users_others_df = pd.read_csv('users_other.csv')\n",
    "print('users_others_df',users_others_df.shape)\n",
    "users_others_df = users_others_df[users_others_df['is_blue_verified'] == True]\n",
    "print('users_others_df',users_others_df.shape)\n",
    "\n",
    "network_df = pd.read_csv('user_following.csv', nrows=10000)\n",
    "print('network_df',network_df.shape)\n",
    "network_df = network_df[network_df['following'].isin(users_others_df['screen_name'])]\n",
    "network_df['following_in_degree'] = network_df['following'].map(network_df['following'].value_counts())\n",
    "print('network_df',network_df.shape)\n",
    "print(network_df.head())\n",
    "network_df = network_df[network_df['following_in_degree'] > 10]\n",
    "print('network_df',network_df.shape)\n",
    "\n",
    "# Initialize the Network\n",
    "net = Network(height=\"750px\", width=\"100%\", directed=True)\n",
    "\n",
    "# Add nodes and edges\n",
    "for index, row in network_df.iterrows():\n",
    "    net.add_node(row['user'], label=row['user'], value=1)\n",
    "    net.add_node(row['following'], label=row['following'], value=row['following_in_degree'])\n",
    "    net.add_edge(row['user'], row['following'])\n",
    "\n",
    "# Generate the graph\n",
    "net.show(\"twitter_network.html\", notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_others_df (28891, 33)\n",
      "users_others_df (5784, 33)\n",
      "network_df (1000, 2)\n",
      "network_df (139, 3)\n",
      "               user        following  following_in_degree\n",
      "20  Chinamission2un      WanmingYang                    2\n",
      "22  Chinamission2un  China_Amb_India                    1\n",
      "23  Chinamission2un   Chinaembmanila                    2\n",
      "27  Chinamission2un       chenweihua                    1\n",
      "30  Chinamission2un    BeijingReview                    1\n",
      "network_df (24, 3)\n",
      "twitter_network.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/kc2mt7ts2934ygy6lzf8rzwr0000gn/T/ipykernel_78978/2644994764.py:5: DtypeWarning: Columns (0,4,5,6,8,10,12,16,17,18,21,22,23,24,25,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  users_others_df = pd.read_csv('users_other.csv')\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "users_df = pd.read_csv('users.csv')\n",
    "users_others_df = pd.read_csv('users_other.csv')\n",
    "print('users_others_df',users_others_df.shape)\n",
    "users_others_df = users_others_df[users_others_df['is_blue_verified'] == True]\n",
    "print('users_others_df',users_others_df.shape)\n",
    "\n",
    "network_df = pd.read_csv('user_following.csv', nrows=1000)\n",
    "print('network_df',network_df.shape)\n",
    "network_df = network_df[network_df['following'].isin(users_df['screen_name'])]\n",
    "network_df['following_in_degree'] = network_df['following'].map(network_df['following'].value_counts())\n",
    "print('network_df',network_df.shape)\n",
    "print(network_df.head())\n",
    "network_df = network_df[network_df['following_in_degree'] > 2]\n",
    "print('network_df',network_df.shape)\n",
    "\n",
    "# Initialize the Network\n",
    "net = Network(height=\"750px\", width=\"100%\", directed=True)\n",
    "\n",
    "added_nodes = set()\n",
    "# Add nodes and edges\n",
    "for index, row in network_df.iterrows():\n",
    "    net.add_node(row['following'], label=row['following'], value=row['following_in_degree'])\n",
    "    added_nodes.add(row['following'])\n",
    "\n",
    "# Add nodes and edges\n",
    "for index, row in network_df.iterrows():\n",
    "    if row['user'] not in added_nodes:\n",
    "        net.add_node(row['user'], label=row['user'], value=1)\n",
    "    net.add_edge(row['user'], row['following'])\n",
    "\n",
    "# Generate the graph\n",
    "net.show(\"twitter_network.html\", notebook=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
